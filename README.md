#  RAG 

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline using LangChain, Qdrant (vector database), and NLP preprocessing.  
It allows users to query custom documents using semantic search and LLM-based contextual answers.

---

##  Overview

The pipeline consists of two main components:

- **`ingest.py`** â€” Loads, preprocesses, and embeds documents, then stores the embeddings in **Qdrant**.  
- **`app.py`** â€” Retrieves relevant document chunks from Qdrant and generates responses using an **LLM** via LangChain.

Qdrant is hosted locally, ensuring fast, private, and efficient vector search.

---

## ğŸ§© Architecture
```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Documents â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”‚
     (1) Ingest
           â”‚
           â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚    Embeddings      â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Qdrant   â”‚  â† Local vector DB
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
     (2) Query
           â”‚
           â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚    LangChain LLM   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Response  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

